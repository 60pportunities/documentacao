"MetricÂ name","Questions these metrics help inform","Metric category"
"AARRR metrics","Focus on acquisition, activation, retention, referrals and revenue as a means of understanding the effectiveness of e-commerce and other sites that prioritize engagement.","Business Outcomes, Value",""
"Actual versus prescribed time for recoveries","Measure the amount of time it takes for the team to address a problem against the amount of time estimated. This metric can be used to better estimate in the future, and to pinpoint features or components that require stronger rollback capabilities.","Value, Flow, Quality",""
"After-hours work","How much paid and unpaid overtime do your employees spend working? By keeping after-hours work to a minimum, you will not only increase business value, but also have happier employees. Keep track of after-hours work, and try to lower the amount as time passes.","Team",""
"Automated build and deployment functionality","This metric tracks the percentage of initiative components that can be built and deployed automatically in a repeatable fashion. Automation increases efficiency, reduces the risk of manual errors and is self-documenting. Your goal should be to continuously increase this percentage.","Flow, Quality
",""
"Automated test functionality","This metric tracks the percentage of initiative components whose functionality can be tested automatically in a repeatable fashion. The goal is to increase this percentage.","Flow, Quality
",""
"Average work in progress","As teams move to visible workflows, they can see the number of work items in each process step. Collect the average work-in-progress metrics by determining the average number of items that exist in each step. Using the average, you can set work-in-progress limits and try to lower the number to a reasonable level.","Team",""
"Backlog size","Number of items on the Product Backlog.  The larger the number the longer the lead times will be.","Team",""
"Blockers","Blocked items highlight dependencies, defects or unavailable skill sets that are preventing progress. Identify blocked items at the daily Kanban meetings by adding a red sticky note on top of the task stating the reason and marking a red dot for each day it is blocked. Escalate the issue if an item remains blocked for three days. Track the number of blocked items each day, and use a retrospective to investigate and resolve root causes.","Flow, Quality",""
"Broken builds","This measurement counts how often build automation fails in a given time period. A broken build is one that does not compile, fails unit tests or is generally unusable by the test team. Broken builds are indicative of sloppy and untrained development teams. The goal is to decrease this number.","Flow, Quality",""
"Burn down/up","Burn-down metrics display a representation of the work left to do versus the time it will take to complete that work. Burn-up metrics measure how much work has been completed. These metrics, popular in agile methodologies, can be used to estimate feature delivery.","Flow",""
"Cadence support","Creating noncoupled components is a best practice because such components are easier to deploy and much less risky. But even noncoupled components have dependencies that must be built and deployed  often at different times with different versions. Use a cadence support metric as a percentage to indicate which component dependencies can support teams working across different methodologies or cadences. For instance, service-oriented architecture (SOA) components have interfaces that can be defined before the components exist, thereby giving other teams the ability to develop  or even release  software in parallel before the components exist. The goal is to increase cadence support by creating components that degrade gracefully or otherwise heal when dependencies dont exist or when infrastructure fails.","Team",""
"Change failure rate (DORA)","The percentage of deployments causing a failure in any dev, test or production environment, or that result in a degraded production service that requires remediation.","Flow, Quality",""
"Change issue severity","Did the change result in data loss, or did it simply delay the rollout? Ranking the severity of change issues can help you pinpoint areas that you need to concentrate on.","Quality",""
"Code coverage","This metric is used to determine how your code was exercised or not during the execution of your test suite. It is frequently used as a measurement of completeness of unit tests. Code coverage can extend into other test types.","Quality",""
"Code quality","Measurements of code quality are generated by static analysis tools and include the codes testability, maintainability, complexity, performance and security.","Quality",""
"Code reviews","This metric measures the percentage of source code that has been peer-reviewed or developed in a pair, mob or swarm development environment. The goal is to increase this number.","Team, Flow, Quality",""
"Component coupling","Use interface characteristics like the numbers of input parameters, output parameters and global variables to create an algorithm for measuring component coupling. Roger Pressman published such algorithms in his book, Software Engineering  A Practitioners Approach. Once you have an algorithm in place, create automation that validates your source codes coupling in a CI environment. The goal is to reduce coupling within your development objects.","Quality",""
"Configuration and infrastructure provisioning automation","Use this metric to track the percentage of components whose configuration and test environments are created through automation. The goal should be to increase this percentage.","Flow, Quality",""
"Continuous testing","This metric measures the percentage of source code that is part of a CI system with full-validation unit tests. The goal is to increase this number.","Flow, Quality",""
"Cost associated with defect","If a defect causes a delay or requires many hours to address, try to determine its associated financial burden. This metric will help teams better categorize defects in the future and allow accountants to determine a projects return on investment. This metric becomes more valuable when correlated with the defect identification metric (described in the next section) because it will expose how long-lived issues cost more to mitigate.","Value",""
"Customer satisfaction","Is your product serving the customers needs and expectations based on direct customer feedback? Does a customer survey reveal thatthe user interface (UI) or user experience (UX) is intuitive, and does the technology perform and feel secure?","Value",""
"Cycle time","This metric records the time spent working on an item or, more technically, when it was pulled into an activity state until it was delivered. Cycle time can also be measured for the individual steps of your SDLC, e.g. design, code, test, deploy. Analyze cycle time to make sense of your process and how it relates to WIP limits. ","Flow",""
"Cyclic dependency","Objects may have a bidirectional dependency  for instance, Object A requires Object B, but Object B also requires Object A. This phenomenon  known as cyclic dependency  prevents objects from being packaged separately and makes them difficult to maintain. Count the number of cyclic dependencies each object has, with the goal of lowering the number to zero.","Flow, Quality",""
"Defect escape rate","Use this metric to measure the number of defects found in different environments (such as development, test, preproduction and production). The goal should be to capture as many defects as possible in the earlier environments and have few defects escape to production systems. This measurement is much easier to ascertain if you use an agreed-upon defect identification table.","Quality",""
"Defect identification","The defect identification metric provides an interval range of values, depending on the phase in which the defect was introduced and the environment in which it was found. For instance, you might use the following linear scale to determine the age of a defect:
Requirement: 1
Design: 4
Development: 8
Test: 16
Preproduction: 32
Production: 64","Quality",""
"Defect leakage","Defect leakage refers to all defects that are found, but not removed, during the respective phase.","Quality",""
"Defects based on misunderstandings","At times, team members report issues that turn out to work as designed. In these cases, you must understand whether the issue emerged due to the reporters lack of experience or judgment. If so, use the opportunity to mentor team members on more accurate methods of testing the product. In the case where the reporter did not understand the functionality, consider whether the user experience needs to be corrected or whether the team member needs education regarding the product domain. The goal is to lower this number over time.","Quality",""
"Dependency and data resiliency","Measure the resiliency of your application components when they do not have access to data or when the database schema doesnt match what the application expects. Your GUI applications typically shouldnt hang or throw exceptions in data failure cases. Instead, they should degrade gracefully by capturing data in a different way or by presenting the user with other options. APIs and other lower-layer components should provide a healthy exception-handling environment when their upstream and data dependencies do not provide necessary information. Count the cases where components proved nonresilient.","Quality",""
"Deployment frequency (DORA)","How often are you able to deploy? How much time does your team take to deploy functionality, based on the time that was requested? The sooner you get functionality to your customers, the sooner your business will see a return of value. The goal is to be able to deliver at the rate that your customer can accept.","Flow",""
"Duplicated code","Using static analysis tools, track the instances of duplicated code so that such code can be refactored into a separate method or object. You must update every instance of duplicated code when defects are found. Its much more efficient to make a single change that then cascades throughout the feature. The goal is to lower this number to zero.","Quality",""
"Dynamic and performance testing","Dynamic analysis tools test applications at runtime by searching for security and performance problems. Load test applications simulate a virtual load on your application to determine performance issues. Use these tools outputs as metrics to find issues. The goal of this metric is to increase the number of issues found early in the development life cycle and to decrease that number as the feature nears deployment.","Quality",""
"Earned business value","Assign a business value point (BVP) to each minimally marketable feature. A minimally marketable feature can be defined as one that is the smallest set of functionality that must be created in order for the customer to perceive value. Add weight to features based on higher levels of value to the business. Once the feature has been deployed, its considered earned business value. BVPs and potential earned business value are helpful for determining which features should be added to the schedule or for determining how much business value development teams have contributed after a product has been deployed.","Value",""
"Effectiveness and user experience","By using analytics to track the time and paths users take to complete tasks, teams find the bottlenecks that frustrate users and the shortcuts they employ to avoid them. Additionally, teams can spot low-usage features that might be discarded. By pruning applications, teams ensure that expensive testing and maintenance resources arent wasted on features that customers do not use. Because this type of data is subjective, teams should collect the data and analyze it to detect patterns for creating more efficient user experiences or to find features that should be removed.","Value",""
"Efficacy","In order for a software application to be successful, the product must allow users to fulfill business transactions. Software that doesnt fulfill a users desire to complete necessary business tasks is ineffective and doomed to fail.","Value",""
"Efficiency","Your test plans should be representative of the change and risk involved in the latest development activity. Rightsizing is essential, and bloat should be avoided. Team efficiency can become severely reduced when priorities are ever-changing. Product owners should watch for lost work and context-switching time resulting from volatile priorities.","Team, Flow",""
"Emergency versus scheduled releases","Measure the percentage of release items that were scheduled, as opposed to those that used an exception policy. The goal is to have a low number of exceptions. If team members are abusing the exception policy, its a sign that the process you offer does not provide people with the tools they need to reach customers within their window of opportunity.","Team, Flow, Quality",""
"Employee turnover and employee satisfaction","Happy employees are more productive and less resistant to process change. Compare time periods against the equivalent times in previous years to understand whether employees are more or less content in their job situations. Use your human resource records to determine whether your employees are generally satisfied with their current employment. Use surveys to determine whether your developers are happy in their roles, share a single purpose, and are motivated to create effective, quality products.","Team",""
"Enforcement of interface contracts & performance/entitlement service-level agreements (SLAs)","As teams move to modern architectures, they need to collect data on whether the individual contracts associated with objects are proper and enforced. As an example, consumers call web API methods or functions that provide a defined set of functionality. Users rely on an APIs interface definition to stay static because changes to the definition cause their applications to break. In a second example, service-level agreements dictate an applications expected performance and availability. In both cases, teams need to measure how often contracts or service levels are not met.","Team, Quality",""
"Entry rate","How many new items are added to the product backlog on a weekly sprint of program increment (PI) basis.","Team",""
"Exit rate","The rate that backlog items are completed.","Team, Flow, Quality",""
"Failing fast","How often do teams stop working on featuresdeemed to have little or no business value? If your organization tends to reward the completion ofnonworthyprojects instead of the courage to recognize when work has little value, you will waste precious worker-hours. Thismetricmeasures how quickly efforts are recognized to be less valuable and how soon they are cancelled.","Value, Team",""
"Feature correctness","This metric measures whether a feature was properly identified and then correctly communicated to the development team. Valuable feature descriptions are consistent, attainable, unambiguous and atomic. Because this metric is subjective, teams must create a scale that describes a features correctness and then use surveys to elicit feedback from the team and/or users. One way to measure correctness is to count removal of, changes to or refinements to features after the development team has begun implementing.","Quality",""
"Feature rejection","Measure the percentage of functionality rejected by test or deployment personnel due to a lack of effectiveness, a deficiency in basic quality or an inadequate level of development testing. The goal is to reduce this number. Ideally, automate basic functionality smoke tests as part of a CI initiative so that test team members dont waste time vetting functionality that isnt ready to be tested.","Quality",""
"Feature reusability","Reusing features allows teams to be more productive by not repeating themselves  the dont repeat yourself (DRY) principle. Well-written features lend themselves to reuse. Use this metric to measure whether a feature is reusable.","Quality",""
"Features tied to business cases","Development teams often waste time and money by creating features that users dont want or need. In some cases, this is caused by a misunderstanding of user requirements. But in other cases, its because developers decide to throw in something because its easy or cool. Once these unnecessary features are integrated into the product, teams must proffer time and effort to build, test and deploy them. To better avoid these scenarios, start measuring the percentage of deployable functionality that hasnt been mapped to an official requirement, user story or business strategy. Traceability features are available through many development tools. The goal is to greatly decrease this number  and to also give teams the chance to reconsider developing or deploying functionality that hasnt been tied to a specific business strategy.","Value, Team",""
"Firefighting","Firefighting is the term used to indicate a single person or group working on unplanned work to save the team from missing a deadline or to fix a production defect. This style of heroism may seem appealing, but its actually a common symptom of a dysfunctional team. Lowering the number of project incidents that require a single group or individual to work overtime allows you to demonstrate your teams ability to work together using an efficient process. This metric is a close relative to the after-hours work metric described in Step 3 of this research.","Team, Quality",""
"(Flow) efficiency","The proportion of time that work items are active against their total cycle time.The flow efficiency in most organizations is very low, typically below 10%, meaning that work is left waiting for 90% of its time.","Flow",""
"Illegal code use","Track the number of times you find either source code that originated from a nonapproved copyrighted location (such as code copied from an open-source project) or a component that depends on a nonapproved library. The goal is to lower this number to zero (and to remove the offending source code from the feature).","Quality",""
"Incidents","These include functional occurrences where action is required to restore or correct data or functionality for end-users. This category can also include security incidents such as violation of confidentiality, integrity or availability of information systems.","Quality",""
"Integration and system test coverage","Integration test coverage is a measurement that describes the degree to which integrated components have been validated together by particular test suites. The definition of coverage is subjective, and teams must create specifications that determine what is and what is not coverage. For instance, a team may decide that full regression test coverage sufficiently validates inputs and outputs for every component. In another example, teams might determine that unit tests must cover every component function, statement and condition. The goal is to specify what coverage means to your organization and then increase the percentage of components that are covered.","Quality",""
"Interface complexity"," Use this metric to measure a components interface by its complexity of maintenance. For instance, a REST API component may have less complexity than one with a brokered web service or one using a Microsoft Component Object Model (COM) interface. By creating components that have less complexity, teams can deploy small bits of functionality more often and with less risk.","Quality",""
"Interface versioning","Use a percentage to indicate the number of components that have defined, versioned interfaces. Versioned interfaces allow teams to maintain different branches of functionality in a way thats seamless to users. The goal is to increase these numbers.","Quality",""
"Issues related to API management and mediation","In the API world, the API management tool is king. Teams must understand how APIs are layered and abstracted, and collect information when issues are related to tool and configuration failures rather than to the API functionality itself.","Quality",""
"Issues related to change","Keep track of deployment issues that have been mapped to specific changes. How large was the change? Was it part of a scheduled release or an exception? Was the problem related to configuration or to an application defect? Keeping track of these metrics allows you to both pinpoint problems and measure your progress.","Quality",""
"Issues related to configuration","This metric relates to the number of issues that occur due to mistakes in configuring the applications environment. Applications are made up of more than just source code. Process and environment configurations play major roles in providing functionality to users. You can better understand the resources and problems related to your environment configurations by tracking associated defects. The goal is to lower this number.","Quality",""
"Issues related to data centers and clouds","This metric measures the number of issues that occur due to data center or cloud downtime, bandwidth limits or cloud provider flaws. Like the metrics associated with configuration issues, this metric helps you pinpoint where you might have data center problems that must be addressed. The goal is to lower this number.","Quality",""
"Issues related to data synchronization","Database and application developers must often work in parallel, and it can be tricky to sync a teams artifacts. Measure the number of issues related to team synchronization. The goal is to lower the number by working more closely together and by employing versioning applications and database schemas.","Quality",""
"Issues related to documentation","Use this number as a way to track issues that resulted from the associated documentation being incomplete or incorrect. The goal of this metric is to decrease the number.","Quality",""
"Issues related to environment parity","This metric measures the number of issues that occur due to differences in environments. Nothing wastes more time than trying to determine a subtle difference in environment that has no correlation to the quality or performance of the product. The goal is to lower this number by identifying streamlined and automated ways to maintain parity between environments.","Quality",""
"Issues related to orchestration","As more teams move toward multigrained mesh app and service architectures (MASA), more and more of the environment must be managed by orchestration tools such as Kubernetes. These tools will collectively start and stop groups of functionality and manage elasticity for applications as demand for functionality grows. Teams must collect metrics on how many issues are related to tool and configuration failures associated with orchestration.","Quality",""
"Issues related to traffic","This metric measures the number of issues that occur due to general performance problems, such as an increase of network traffic or application use. How your customers perceive your product is directly related to how well it performs. By breaking out performance metrics, you can better understand where you might need to increase expertise or expensive tuning resources. The goal is to lower this number.","Quality",""
"Lead time","This metric is the duration between the commitment and delivery dates  that is, the time taken for an item to move across your Kanban board. Ideally, this is the customer lead time from the initial agreement to deployment, but it is often simply from when your team accepts the kanban to when it delivers the resultant work. Count the calendar days (including weekends and holidays) to represent your customers view of your development time.","Value",""
"Leadtime for changes (DORA)","How long does it take from code committed to running in production? High performers achieve this in less than one hour.","Flow","This is really cycle time."
"Life span of branches","Branches should be created for rapid changes only and be merged quickly back into the mainline of code. Measure each branchs time in minutes/hours, and use this metric to work toward a goal of making branches short-lived (for example, half a day long).","Flow, Quality",""
"Manual rework","Count the number of times automation failed, requiring team members to manually intercede to continue the pipeline process. The goal is to refactor your scripts so that they perform better in order to lower this number.","Flow, Quality",""
"Market timing","A product will fail if it isnt released within an appropriate window of business opportunity. That window might be based on competition, such as beating competitors to market, or it might be based on providing the right tool when the user needs it.","Value",""
"Mean time between failures(MTBF)","MTBF is the arithmetic mean time between failures of a technology product.","Quality",""
"Mean time to detect (MTTD)","Teams use this metric to measure their ability to quickly discover problems that occur on production systems. The time variable measures the period between the onset of a revenue-impacting event and the teams detection of that event. The goal is to decrease this number. Teams should strive to keep this number under 24 hours.","Quality",""
"Mean time to recover (MTTR)","How long it takes an organization to recover from a failure due to deployment, or a system failure.","Quality",""
"Mean time to resolve (MTTR)"," Similar to MTTD, teams use this metric to calculate the average time period between the occurrence of an incident and the satisfactory resolution of that incident. The goal is to decrease this number.","Quality",""
"Mean time to restore (DORA)","How long does it take from service interruption to restoration? High performers take less than one hour to restore from a service incident.","Quality",""
"Net Promoter Score","How loyal are your customers to the brand your product or team is generating? Have your customers become advocates for your product or service and do they recommend it to others?","Value",""
"Number and severity of defects","Use this metric to track the overall number and severity of defects in a feature. An alternative form of this metric is to measure defect density. Defect density is the number of defects found in a specific time period divided by the size of the module (for example, lines of code).","Quality",""
"Number of commits","Developers should commit code to version control systems often to ensure that automated systems fully integrate source code in a continuous manner. Use this metric to track how often developers commit code to the version control system. The goal is to increase this number and to ensure that developers commit at least once per day.","Flow, Quality",""
"Number of features with associated version control, issue management and deployment tools","Proper tooling is essential for ensuring features meet efficacy, quality and timing requirements. Be aware that many teams falsely see tools as a silver bullet. Although they can help you solve your problems, tools wont help you if your team lacks self-reflection and the ability to improve. Your goal should be to look for process steps where a tool might replace or improve manual interactions.","Vaue, Team",""
"Number of parameters","Count each methods number of parameters. A large number of parameters makes for confusing development. If the number of parameters for a method is larger than four, consider a requirement for passing objects instead.","Quality",""
"Number of steps in the process requiring authentications","A surplus of authentication gates inherently slows your team down and makes advanced techniques like continuous delivery nearly impossible to implement. Document and continuously revisit the number of authentication steps in your processes in order to keep the number low and to ensure that there are compelling reasons for their use.","Vaue, Team",""
"Output as a ratio of expertise","Its difficult to know the level of expertise necessary for new hires. By measuring the ratio between successful outcomes and the staffs current expertise levels, you can more easily determine the type of hires you might want in relation to upcoming development requirements. This metric is subjective and difficult to capture, so use it with caution","Team",""
"Output relative to effort","Teams must be able to spot low-hanging fruit that provides high business value for low effort. Conversely, teams want to avoid working on functionality that requires a high workload with low payoff. Teams can predict what functionality might have a big bang for the buck by tracking and referring to historical measures. Track this metric as a ratio of business value points to worker hours.","Team",""
"Personnel necessary for deployment operations","How many people and how much time does it take to deploy your product? High numbers here indicate the need to streamline your process and introduce more automation. The goal is to continually lower this number in future cycles","Team",""
"Planned test versus actual tests","Some test plans dont finish the way they started. Why? Are your teams missing dependencies in the test environment? Are automated tests becoming flaky and requiring more dedicated maintenance time? Measure to understand.","Flow, Quality",""
"Platform or architecture adherence","Architecture reviews, and static and compositional analysis tools and methods help to measure the implementation of a technology compared to suggested templates, reusable components, and suggested dependencies and services. This can later assist in better managing technical debt and modernization efforts.","Quality",""
"Portability","This metric is a measure of how well-prepared an object is to change to new specifications, operating systems or platforms. This metric corresponds to your current platforms and also extends to platforms you may choose to support in the future. For example, you may have a service object that lives in a Java application server environment. You can support that object on any operating system that has an applicable application server. What happens when you want to move it to the cloud? Does it already have some cloud characteristics?","Quality",""
"Product-team health","Measurement of job satisfaction among the testers and developers within your software engineering team. This can be conducted through worker surveys and one-to-one meetings.","Team",""
"Published APIs","APIs give your users and developers the means to create functional applications without having to understand the intricacy and algorithms of back-end systems. Measure and collect data about the number of systems that have published APIs and the percentage of functionality they expose in contrast to your graphical interfaces.","Vaue, Team",""
"Ratio of processes that are repeatable","A repeatable process is defined as one where you achieve the same result every time you complete it using the same inputs. Repeatable processes are key to any automation effort because they are reliable and predictive. Your goal should be to make all of your processes repeatable.","Vaue, Team",""
"Regulation","Assess the number of government or corporate regulations to which projects must adhere. Highly regulated projects may require more manual steps, and manual steps make those projects less-suited for continuous deployment. Alternatively, they may have documentation requirements, which make them well-suited for automatic delivery mechanisms. Qualify your regulated projects using a 10-point scale based on the applicability of an automated build and delivery system.","Value, Team",""
"Release cadence","Measurement of frequency of release as measured by number of releases over a period of time (for example, four releases per month). Frequency of which features are delivered to a product.","Flow",""
"Release success rate","Collect information on whether the team was able to meet its goals for each application release. Did it meet its date? Was all the required functionality included? Was the quality acceptable? Rate each of these factors (and others that you might deem important) using a 10-point scale, with the goal of releasing more successfully in future cycles.","Quality",""
"Reliability","Although the term quality is often conflated with the word reliability, true product quality encompasses all of the differing aspects of software. But dont let such quibbles about nomenclature lead you to think that reliability isnt important. Users cannot exploit an effective product if it consistently crashes or suffers from performance and security problems.","Value, Quality",""
"Removal of unnecessary code","By tracing released, unnecessary features back to source code, teams can cull functionality from builds and testing, thereby saving the team effort and technical resources. Identify and count incidents of unnecessary source code that have been retired from the initiative.","Quality",""
"Retention of testing expertise or engineers","Measurement of employee retention rate within the product team. Measure retention of skills and not simply a headcount of staff. This will better inform your hiring and training plans.","Team",""
"Return on investment (ROI)","Teams measure initiative earnings and value against expenditures to determine the ROI delivered by products. Comparing the ROI history of past features allows analysts to better formulate future business strategies. The formula for determining ROI is to:
1. Measure the end-dollar value of functionality created.
2. Divide that dollar value by the amount of money spent to create it.","Value",""
"Reusable components","Measure the number of components that are available as reusable objects in your development system. By creating components that can be used in more than one circumstance, maintenance and feature addition becomes easier because changes cascade to all the locations that are using it. The goal is to increase this number.","Team",""
"Reusable test logic","This metric measures the amount of test logic that can be built atomically and that can support different kinds of testing. For instance, by using dynamic injection, teams can employ unit tests for both isolation and integration testing. In another example, a single functional Selenium test might also be used for performance testing. The goal is to increase this number.","Team",""
"Revenue","Income earned from providing a service or product to customers. Revenue is measured through profitability. Common metrics used include net profit margin and monthly recurring revenue.","Value",""
"Scope creep","Scope creep is the number of changes that sneak into the development process after an initial agreement has been met about an initiatives cost and scope. Many people think that agile methodologies  which have been designed to embrace change  should allow for never-ending scope creep. In many ways, thats true: In an agile world, its not scope creep if no one has started work yet or if the team allocated time to this particular unknown. True scope creep, using any methodology, is anything that requires additional work when nothing else has been deprioritized. Use this metric to measure when new work was prioritized without deprioritizing something else. The goal is to decrease scope creep.","Team",""
"Separation of concerns","A category divides classes into distinctive groups with shared characteristics  such as libraries, hierarchies or external interactions. A team from the University of Catania published a paper, Metrics for Evaluating Separation and Composition, which contains algorithms to measure architectural separation of concerns. Much like with component coupling, use automation to determine whether source code has had its concern properly separated as part of a CI effort. Finding architectural issues early means the team can fix problems before objects become actual dependencies.","Quality",""
"Service-level indicators (SLIs) or service-level objectives (SLOs)","SLIs indicate the level of service that you are providing and can reference latency, throughput, or response times among other characteristics. SLOs specify a target level for the reliability of your service.","Quality",""
"Skills assessment","As the technology environment continues to evolve, learning is an important activity for software engineers and testers. For key areas of responsibilities, utilize skill assessments to drive your investments in conference participation, training and certification, and team programming or pairing work.","Team",""
"Smoke test escape"," Use this metric to determine how many blocking bugs escape your basic smoke tests. The goal is to decrease this number.","Quality",""
"Source code cyclomatic complexity","This metric gauges the complexity of a program by measuring the number of linearly independent paths logic can take. Continuous inspection tools  such as open-source SonarQube  measure such complexity. Complex code is difficult to maintain and test, and it is more likely to contain defects. The goal is to decrease the complexity of your applications to an acceptable level.","Quality",""
"Source code standards","Use this metric to measure, as a percentage, the amount of source code that has been analyzed to be compliant with the organizations coding style standards  such as those that address indent spacing, documentation requirements and naming conventions. The goal is to increase this number.","Quality",""
"Spoilage","This metric allows teams to aggregate escape rates into a single number that can be monitored as a trend. To determine spoilage, multiply the defect identification by the number of defects found in that environment, and then divide that total by the total number of defects found in all phases. You can add the spoilage numbers for each phase together to see an aggregate total that indicates your total spoilage level.","Quality",""
"Stabilization versus development (aka fixes versus features)","Use your issue manager to determine how many changes were made to deliver new features versus to stabilize and maintain existing functionality. Your goal is to deliver new, compelling functionality. If you find that you spend too much time stabilizing your product before release, address how your team currently integrates source code and modify your processes in ways that will integrate your source code more appropriately. See Gartners "A Guidance Framework for Continuous Integration: The Continuous Delivery Heartbeat" and "Solution Path for Achieving Continuous Delivery With Agile and DevOps" for more information about how to streamline your teams source code integration and how to start a continuous integration initiative.","Flow, Quality",""
"Staff allocation","Development initiatives are easier to implement if your teams are consistent and stable instead of being completely reshaped with every feature incarnation. Use a percentage to indicate the number of your stable teams. A variation of this metric is to compare allocation time with the amount of time employees actually spend working on the initiative. This is a good measurement of cultural stability.","Team",""
"Static code analysis: ","This metric measures the percentage of source code that has been analyzed for improper coding patterns by static analysis, security and algorithm validation tools. The goal is to increase this number.","Quality",""
"System WIP","During the daily Kanban meeting, write down the number of cards between the commitment and delivery points (started but not finished) and look for a decrease as WIP limits are reduced. If the number of active kanban cards in your system exceeds twice the number of people working on them, it is an indication of inefficient multitasking that can hamper productivity.","Team",""
"Team distribution","When your teams are distributed, it becomes harder for them to collaborate. Measure your percentage of distributed versus colocated teams as an indicator of team cohesiveness. Augment this metric with details about time zone differences, language and/or cultural barriers that the teams face.","Team",""
"Team incentives and empowerment","Common incentives and group empowerment focus team members on what they can achieve together rather than pitting them against each other. An empowered team is one that is accountable and trusted to adjust how they work in order to deliver better outcomes. See Create Awesome Software Using Agile Practices for more information on what constitutes an empowered team.","Team",""
"Team member load and capacity","By tracking individual member capacities and load, teams can gauge progress, avoid bottlenecks and manage resources more effectively. Such measurements also help teams estimate properly in the future. The goal with this metric is not to change the teams volume of work, but to collect the data for historical analysis.","Team",""
"Test cycle times","Use this metric to measure the amount of time each test cycle takes for specific products, components and classes. The goal is to understand how long test cycles take for estimation purposes.","Quality",""
"Test efficacy"," Use this metric to determine the type of tests that found issues. For instance, how many issues were found due to unit testing versus regression testing? The goal is to try to find more issues during the development cycle than during late-phase testing.","Quality",""
"Test reviews","This metric measures the percentage of test code (for both development unit tests and standard quality tests) that has been peer-reviewed or developed in a pair, mob or swarm development environment. The goal is to increase this number.","Quality",""
"Test types","Many teams only run UI tests. This is insufficient.Measuring test types can help software engineering leaders visualize a lack of layered tests. Organizations often find that they are omitting API tests or struggling to adopt automation or nonfunctional test types","Team, Flow, Quality
",""
"Testing costs and investments","The investments in testing and quality commonly include certification and training, licensing for commercial products, SaaS and cloud consumption during test execution, conference participation, labor, and more.","Value, Team, Quality",""
"Throughput","The number of work items completed in a set time period. It is an advantage to standardize work item size to reduce variance, but it will still average over a longer period.","Flow",""
"Transactions","Transactions may differ widely based upon the applications purpose and can include number of site visits, cart-size, conversion rates, process completion, request or error rates, among others.","Value",""
"Trunk-based development","Trunk-based development allows developers to develop and update functionality in isolation by using toggle switches within the application itself to deny users access to components during construction. This branching model makes frequently deploying easier. Measure the percentage of components in your project that use a single mainline of code. The goal is to be able to release more frequently by avoiding the merging of large branches.","Flow, Quality",""
"Unit test coverage","Use this metric to state, as a percentage, the number of components that have full-validation unit test coverage. The goal is to increase this number.","Flow, Quality",""
"Unplanned work due to poor quality","Unplanned work causes us to break promises regarding planned work. Determining theroot causesof unplanned work  whether it is related to requirements,process or code defects  helps the team remove constraints and work more predictably.","Flow, Quality",""
"Unused or dead code","Using static analysis tools, track instances of existing application code that cant be called through any available pathways. Unused code bloats the pipeline and might introduce security risks. The goal is to lower this number to zero.","Quality",""
"User adoption","Teams use application performance monitoring (APM) tools to count the unique new users of a product or feature in order to validate feature strategies and marketing materials. Because what might be considered good for this value is typically subjective, teams use formulas that compare a time period for adoption values against equivalent time periods in the past. As an example, a textbook sales team might measure the user adoption rate for this years September against last years September adoption rate to determine whether new features are attracting users to the product.","Value",""
"User engagement","Teams use APM tools to count the number of returning users of a product or feature during a specific period of time. Such data is useful to predict load and to determine whether the product continues to be effective or has quality issues. User engagement formulas employ similar measurement techniques to those used for user adoption.","Value",""
"User retention and churn","Teams use APM tools and databases to monitor how many active users from a given time period are still present at a later time period. Teams use such data in order to determine whether the product is effective or has quality issues. User retention formulas employ similar measurement techniques to those used for user adoption.","Value",""
"User satisfaction","In addition to the above metrics, teams might use more subjective techniques, such as user surveys, to determine whether customers are happy with the effectiveness and quality of a product or feature. Net Promoter Score (NPS) is a standard that teams use to calculate user satisfaction.","Value",""
"Velocity","Velocity tracks the amount of work completed in each sprint by a scrum team. To be counted, work must pass its acceptance criteria and meet the definition of done.","Flow",""
"WCAG/accessibility","Web Content Accessibility Guidelines (WCAG) 2 is developed through the World Wide Web Consortium (W3C) process, through community and collaborative engagement. The intention is to provide a standard for web content accessibility.1Tooling and methods can score technology based upon this standard.","Value, Quality",""
"Work in progress","The number of work items that have been started and remain either active or waiting. An increase in work in progress is a leading indicator of reduced throughput.","Flow",""
"Work item distribution","Are API automated tests only built by a single team or teammate? Do you have knowledge silos on the team or a hero mentality when it comes to pushing teams or releases across the line?","Flow",""
"Work profile","The proportion of each type of work item delivered in a time period by the software value stream. Balancing work is essential to effective product management.","Team, Flow",""
"Zero-downtime deployment","Use this metric to track the number of components that can be deployed while the application is still available to customers in order to avoid disruption. This style of deployment requires very loose coupling and strong separation of concerns so that no one function can bring down everything. By deploying your applications without shutting down, you dont inconvenience your customers. The goal is to increase this number.","Value",""
